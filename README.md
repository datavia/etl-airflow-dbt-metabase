# End-to-End Data Engineering Project  
### Apache Airflow Â· dbt Â· PostgreSQL Â· Metabase Â· Docker

---

## ğŸ§© Overview

This project showcases a **modern end-to-end ELT (Extractâ€“Loadâ€“Transform)** data pipeline built with:

- **Apache Airflow** â€“ workflow orchestration and scheduling  
- **dbt (Data Build Tool)** â€“ SQL-based data transformation and testing  
- **PostgreSQL** â€“ data warehouse for raw and modeled data  
- **Metabase** â€“ data visualization and dashboarding  
- **Docker Compose** â€“ containerized, reproducible environment  

It demonstrates how raw data can be ingested, transformed, and visualized through a fully automated pipeline â€” a typical workflow used in real-world data-engineering projects.

> The original version ran on a cloud VM with S3-compatible storage (Yandex Object Storage).  
> For demonstration, it now uses a simplified local setup that can easily be adapted or extended.

---

## âš™ï¸ Architecture

```mermaid
flowchart TD
    DockerHub[(ğŸ›³ï¸Docker Hub Image Repository)] -->|Pull images| DockerCompose
    GitHub["ğŸ“‚GitHub Repository"] -->|Sync DAGs| GitSync

    subgraph DockerCompose["ğŸ› ï¸Docker Compose"]
        Webserver[ğŸ–¥ï¸Airflow Webserver]
        Scheduler[â°Airflow Scheduler]
        GitSync["ğŸ”„GitSync "]
        DBTDocs["ğŸ“„dbt Docs"]
    end

    GitSync --> Webserver
    GitSync --> Scheduler

    Scheduler -->|schedules| DAG
    Webserver -->|view/trigger| DAG

    subgraph DAG["âš™ï¸Airflow DAG (@hourly)"]
        direction LR
        DAGStart([â–¶ï¸Start DAG]) --> Task1_LoadS3["â¬‡ï¸Task 1: Load Raw Data from Yandex S3 (STG)"] --> Task2_DBT["âš¡Task 2: Run DBT models (ODS, Marts)"] --> DAGEnd([â¹ï¸End DAG])
    end

    YandexS3[(â˜ï¸Yandex Cloud S3 Storage)] -->|Provide raw data| Task1_LoadS3

    Task2_DBT --> PostgreSQL[(ğŸ—„ï¸PostgreSQL Database)]

    PostgreSQL -->|Read marts data| Metabase["ğŸ“ŠMetabase (Dashboards)"]

    style DAG fill:#f9f9f9,stroke:#333
```


